{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate fine-tuned sensationalist detector model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load sensationalist detector for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-01 21:42:18.285966: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-01 21:42:18.310999: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-01 21:42:18.710603: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Llama patching release 2024.6\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3060. Max memory: 11.754 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.1+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.26.post1. FA = False.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Unsloth 2024.6 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "max_seq_length = 2048 \n",
    "dtype = None \n",
    "load_in_4bit = True\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"llama-3-8b-bnb-4bit-sft-sensationalist-detector-v2_model\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "FastLanguageModel.for_inference(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format testset as the instructions from the training set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\" \n",
    "Below you will find the title, summary, and body of a news article. \n",
    "Your task is to analyze these components and classify whether the article is sensationalist or not.\n",
    "\n",
    "Sensationalist is defined as: \"presenting information in a way that is intended to provoke public interest, excitement, or anxiety, at the expense of accuracy.\"\n",
    "\n",
    "### Article information:\n",
    "    Title: {}\n",
    "    Subheading: {}\n",
    "    Body: {}\n",
    "    is this article false?: {}\n",
    "    \n",
    "### is this article sensationalist?:\n",
    "{}\n",
    "\"\"\"\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "    \n",
    "    title = examples[\"Titular\"]\n",
    "    summary = examples[\"Copete\"]\n",
    "    body      = examples[\"Cuerpo\"]\n",
    "    is_sensationalist      = examples[\"Amarillismo\"]\n",
    "    is_false = examples[\"Falsa\"]\n",
    "    texts = []\n",
    "    for title, summary, body, is_false, is_sensationalist in zip(title, summary, body, is_false, is_sensationalist):\n",
    "        # Must add EOS_TOKEN\n",
    "        text = prompt.format(title, summary, body, is_false, is_sensationalist) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions for inference, load testset and get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def inference(article_data, model, tokenizer):\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "    [\n",
    "        prompt.format(\n",
    "            article_data[\"Titular\"], # titular\n",
    "            article_data[\"Copete\"], # summary\n",
    "            article_data[\"Cuerpo\"],# body\n",
    "            article_data[\"Falsa\"], # is false\n",
    "    \"\"\n",
    "    )], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
    "    return tokenizer.batch_decode(outputs)\n",
    "\n",
    "import pandas as pd\n",
    "def load_dataset(dataset_path):\n",
    "    dataset = pd.read_json(dataset_path, orient='index',  encoding='latin1')\n",
    "    return dataset\n",
    "\n",
    "def text_generated2label(text_generated):\n",
    "    is_sensationalist_answer = text_generated[text_generated.find(\"### is this article sensationalist?:\\n\") + len(\"### is this article sensationalist?:\\n\"):]\n",
    "    if(\"No Amarillista\" in is_sensationalist_answer):\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = pd.read_csv(\"data/testset_realnews.csv\", encoding='latin1')\n",
    "testset[\"Amarillismo_binary\"] = testset[\"Amarillismo\"].apply(lambda x: 0 if x == \"No Amarillista\" else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(model_name, model, tokenizer, dataset):\n",
    "    predictions = pd.DataFrame(columns=[\"Id\",\"Predicted_label\", \"True_label\", \"Generated_text\"])\n",
    "    for i in range(len(dataset)):\n",
    "        article_data = dataset.iloc[i]\n",
    "        text_generated = inference(article_data, model, tokenizer)\n",
    "        label = text_generated2label(text_generated[0])\n",
    "        predictions.loc[len(predictions)] = [article_data[\"Id\"], label, article_data[\"Amarillismo_binary\"], text_generated[0]]\n",
    "\n",
    "    predictions.to_csv(f'predictions_{model_name}.csv', index=False)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "predictions_llama_sft_sensationalist = make_predictions(\"llama-3-8b-bnb-4bit-sft-sensationalist-detector-v2-testset\", model, tokenizer, testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, balanced_accuracy_score\n",
    "\n",
    "def generate_report(predictions):\n",
    "    report = classification_report(predictions[\"True_label\"], predictions[\"Predicted_label\"], labels=[0,1], target_names=[\"No Amarillista\", \"Amarillista\"], output_dict=True)\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    \n",
    "    balanced_accuracy = balanced_accuracy_score(predictions[\"True_label\"], predictions[\"Predicted_label\"])\n",
    "    print(\"Balanced accuracy: \" + str(balanced_accuracy))\n",
    "    return report_df, balanced_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report for sft model on testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy: 0.6286764705882353\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>No Amarillista</th>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.514286</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amarillista</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.638298</td>\n",
       "      <td>17.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.585366</td>\n",
       "      <td>0.585366</td>\n",
       "      <td>0.585366</td>\n",
       "      <td>0.585366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.659091</td>\n",
       "      <td>0.628676</td>\n",
       "      <td>0.576292</td>\n",
       "      <td>41.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.686253</td>\n",
       "      <td>0.585366</td>\n",
       "      <td>0.565705</td>\n",
       "      <td>41.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                precision    recall  f1-score    support\n",
       "No Amarillista   0.818182  0.375000  0.514286  24.000000\n",
       "Amarillista      0.500000  0.882353  0.638298  17.000000\n",
       "accuracy         0.585366  0.585366  0.585366   0.585366\n",
       "macro avg        0.659091  0.628676  0.576292  41.000000\n",
       "weighted avg     0.686253  0.585366  0.565705  41.000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report, balanced_accuracy_score = generate_report(predictions_llama_sft_sensationalist)\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eval sft model on trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = load_dataset(\"data/trainset.json\")\n",
    "trainset[\"Id\"] = trainset.index\n",
    "trainset[\"Amarillismo_binary\"] = trainset[\"Amarillismo\"].apply(lambda x: 0 if x == \"No Amarillista\" else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "predictions_llama_sft_sensationalist_trainset = make_predictions(\"llama-3-8b-bnb-4bit-sft-sensationalist-detector-v2-trainset\", model, tokenizer, trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy: 0.905940594059406\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>No Amarillista</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.811881</td>\n",
       "      <td>0.896175</td>\n",
       "      <td>101.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amarillista</th>\n",
       "      <td>0.841667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.914027</td>\n",
       "      <td>101.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.905941</td>\n",
       "      <td>0.905941</td>\n",
       "      <td>0.905941</td>\n",
       "      <td>0.905941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.920833</td>\n",
       "      <td>0.905941</td>\n",
       "      <td>0.905101</td>\n",
       "      <td>202.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.920833</td>\n",
       "      <td>0.905941</td>\n",
       "      <td>0.905101</td>\n",
       "      <td>202.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                precision    recall  f1-score     support\n",
       "No Amarillista   1.000000  0.811881  0.896175  101.000000\n",
       "Amarillista      0.841667  1.000000  0.914027  101.000000\n",
       "accuracy         0.905941  0.905941  0.905941    0.905941\n",
       "macro avg        0.920833  0.905941  0.905101  202.000000\n",
       "weighted avg     0.920833  0.905941  0.905101  202.000000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report, balanced_accuracy_score = generate_report(predictions_llama_sft_sensationalist_trainset)\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hallucinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_hallucinations(predictions):\n",
    "    \n",
    "    hallucinations_responses_idx = [] \n",
    "    for i in range(len(predictions)):\n",
    "        response = predictions['Generated_text'][i]\n",
    "        response_is_sensationalist = response[response.find(\"### is this article sensationalist?:\\n\") + len(\"### is this article sensationalist?:\\n\"):]\n",
    "        \n",
    "        if \"No Amarillista\" not in response_is_sensationalist  and \"Amarillista\" not in response_is_sensationalist:\n",
    "            print(response_is_sensationalist)\n",
    "            hallucinations_responses_idx.append(predictions.iloc[i].name)\n",
    "            \n",
    "    print(f\"Number of hallucinations {len(hallucinations_responses_idx)} out of {len(predictions)} predictions.\")\n",
    "    return hallucinations_responses_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hallucinations 0 out of 41 predictions.\n",
      "Number of hallucinations 0 out of 202 predictions.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Detect hallucinations in the predictions in testset\n",
    "detect_hallucinations(predictions_llama_sft_sensationalist)\n",
    "\n",
    "# Detect hallucinations in the predictions in trainset\n",
    "detect_hallucinations(predictions_llama_sft_sensationalist_trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval base model without fine-tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-01 21:57:04.035738: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-01 21:57:04.061084: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-01 21:57:04.465294: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Llama patching release 2024.6\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3060. Max memory: 11.754 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.1+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.26.post1. FA = False.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "max_seq_length = 2048 \n",
    "dtype = None \n",
    "load_in_4bit = True\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redefine prompt to get a yes or no answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\" \n",
    "Below you will find the title, summary, and body of a news article. \n",
    "Your task is to analyze these components and classify whether the article is sensationalist or not.\n",
    "\n",
    "Sensationalist is defined as: \"presenting information in a way that is intended to provoke public interest, excitement, or anxiety, at the expense of accuracy.\"\n",
    "\n",
    "### Article information:\n",
    "    Title: {}\n",
    "    Subheading: {}\n",
    "    Body: {}\n",
    "    is this article false?: {}\n",
    "    \n",
    "### is this article sensationalist? (Answer with Yes or No):\n",
    "{}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#redefining the function to work with the new prompt format\n",
    "def text_generated2label(text_generated):\n",
    "    is_sensationalist_question = \"### is this article sensationalist? (Answer with Yes or No):\\n\"\n",
    "    is_sensationalist_answer = text_generated[text_generated.find(is_sensationalist_question) + len(is_sensationalist_question):]\n",
    "    if(\"Yes\" in is_sensationalist_answer):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "predictions_llama_base_testset = make_predictions(\"llama-3-8b-bnb-4bit-testset\", model, tokenizer, testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy: 0.4276960784313726\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>No Amarillista</th>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amarillista</th>\n",
       "      <td>0.366667</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.468085</td>\n",
       "      <td>17.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.390244</td>\n",
       "      <td>0.390244</td>\n",
       "      <td>0.390244</td>\n",
       "      <td>0.390244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.410606</td>\n",
       "      <td>0.427696</td>\n",
       "      <td>0.376900</td>\n",
       "      <td>41.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.418108</td>\n",
       "      <td>0.390244</td>\n",
       "      <td>0.361331</td>\n",
       "      <td>41.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                precision    recall  f1-score    support\n",
       "No Amarillista   0.454545  0.208333  0.285714  24.000000\n",
       "Amarillista      0.366667  0.647059  0.468085  17.000000\n",
       "accuracy         0.390244  0.390244  0.390244   0.390244\n",
       "macro avg        0.410606  0.427696  0.376900  41.000000\n",
       "weighted avg     0.418108  0.390244  0.361331  41.000000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report, accuracy = generate_report(predictions_llama_base_testset)\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_hallucinations_base_model(predictions):\n",
    "        \n",
    "    hallucinations_responses_idx = [] \n",
    "    for i in range(len(predictions)):\n",
    "        text_generated = predictions['Generated_text'][i]\n",
    "        is_sensationalist_question = \"### is this article sensationalist? (Answer with Yes or No):\\n\"\n",
    "        is_sensationalist_answer = text_generated[text_generated.find(is_sensationalist_question) + len(is_sensationalist_question):]\n",
    "        \n",
    "        if(\"Yes\" not in is_sensationalist_answer and \"No\" not in is_sensationalist_answer):\n",
    "            hallucinations_responses_idx.append(predictions.iloc[i].name)            \n",
    "    print(f\"Number of hallucinations {len(hallucinations_responses_idx)} out of {len(predictions)} predictions.\")\n",
    "    return hallucinations_responses_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hallucinations 39 out of 41 predictions.\n"
     ]
    }
   ],
   "source": [
    "hallucinations_idx = detect_hallucinations_base_model(predictions_llama_base_testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|> \n",
      "Below you will find the title, summary, and body of a news article. \n",
      "Your task is to analyze these components and classify whether the article is sensationalist or not.\n",
      "\n",
      "Sensationalist is defined as: \"presenting information in a way that is intended to provoke public interest, excitement, or anxiety, at the expense of accuracy.\"\n",
      "\n",
      "### Article information:\n",
      "    Title: Impresionantes hallazgos en el espacio y en las aguas del golfo de MÃƒÂ©xico\n",
      "    Subheading: nan\n",
      "    Body: Impresionantes hallazgos en el espacio y en las aguas del golfo de MÃƒÂ©xico\n",
      " La NASA descubriÃƒÂ³ una reconexiÃƒÂ³n magnÃƒÂ©tica que tendrÃƒÂ¡ relevancia en futuros estudios sobre el Sol. AdemÃƒÂ¡s, la Oficina Nacional de AdministraciÃƒÂ³n OceÃƒÂ¡nica y AtmosfÃƒÂ©rica de Estados Unidos explorÃƒÂ³ las aguas del golfo de MÃƒÂ©xico y descubriÃƒÂ³ nuevas especies.\n",
      "    is this article false?: Verdadera\n",
      "    \n",
      "### is this article sensationalist? (Answer with Yes or No):\n",
      "\n",
      "Yes\n",
      "<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "print(predictions_llama_base_testset.iloc[hallucinations_idx[4]][\"Generated_text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
